<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2025-2-22</title>
    <link href="/2025/02/22/2025-2-22/"/>
    <url>/2025/02/22/2025-2-22/</url>
    
    <content type="html"><![CDATA[<h1id="efficient-prompt-compression-with-evaluator-heads-for-long-context-transformer-inference"><ahref="https://www.arxiv.org/abs/2501.12959v2">Efficient PromptCompression with Evaluator Heads for Long-Context TransformerInference</a></h1><h2id="本文提出一种名为-ehpc-的无需训练的-prompts-压缩方法通过注意力分数筛选出特定的注意力头作为评估头识别出-prompts-中更重要的-tokens实现时间复杂度更低的压缩">本文提出一种名为EHPC 的无需训练的 prompts压缩方法，通过注意力分数筛选出特定的注意力头作为评估头识别出 prompts中更重要的 tokens，实现时间复杂度更低的压缩。</h2><h2 id="方法">方法</h2><h3id="使用-needle-in-a-haystack-benchmark-作为训练资料该资料在文本中标记-needle-tokens重要的文本通过-needle-tokens-的累计注意力得分的求和来选出平均得分最高的一层中得分最高的几个注意力头作为评估头">使用<ahref="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">Needle-in-a-Haystackbenchmark</a> 作为训练资料，该资料在文本中标记“ Needletokens”（重要的文本），通过 Needle tokens的累计注意力得分的求和来选出平均得分最高的一层中得分最高的几个注意力头作为评估头。</h3><h3 id="section"><img src="image.png" /></h3><h3id="选出评估头后计算待压缩-prompts-中的-tokens-的平均注意力分数删去分数较低的-tokens同时为了缓解直接删除后上下文元素会缺失的现象文中使用了-snapkv-llm-knows-what-you-are-looking-for-before-generation-中的-pooling">选出评估头后，计算待压缩prompts 中的 tokens 的平均注意力分数，删去分数较低的tokens，同时为了缓解直接删除后上下文元素会缺失的现象，文中使用了 <ahref="https://openreview.net/forum?id=poE54GOq2l">SnapKV: LLM Knows WhatYou are Looking for Before Generation</a> 中的 pooling。</h3><h2 id="对评估头的研究">对评估头的研究</h2><ol type="1"><li>在长语境中，中间的 layer的一些分布稀疏的注意力头有助于识别与任务相关的信息</li><li>评估头的性能优于对照组（文中没做只用评估头和完整模型的性能对比）</li><li>使用用户化任务确定的评估头并不能显著提高下游任务的性能，即评估头与任务无关，从简单的QA 数据中识别出的评估头就挺好用的。</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-2-21</title>
    <link href="/2025/02/21/2025-2-21/"/>
    <url>/2025/02/21/2025-2-21/</url>
    
    <content type="html"><![CDATA[<h1 id="active-retrieval-augmented-generation"><ahref="https://arxiv.org/abs/2305.06983">Active Retrieval AugmentedGeneration</a></h1><h2id="本文介绍了一种-rag-方法-flare-它在生成过程中主动决定何时检索和检索什么之前的文章中使用以下两种方法-1.-每-n-个-token-就检索一次-2.-将问题分解为多个问题逐个检索-flare-分为-flare_instruct-和-flare_direct使用了-in-context-retrieval-augmented-language-models-中的方法来构建检索文档">本文介绍了一种RAG 方法 FLARE，它在生成过程中主动决定何时检索和检索什么。之前的文章中使用以下两种方法：1. 每 n 个 token 就检索一次； 2. 将问题分解为多个问题逐个检索。 FLARE分为 <span class="math inline">\(FLARE_{instruct}\)</span> 和 <spanclass="math inline">\(FLARE_{direct}:\)</span>，使用了 <ahref="https://arxiv.org/abs/2302.00083">In-Context Retrieval-AugmentedLanguage Models</a> 中的方法来构建检索文档。</h2><h2 id="flare_instruct"><spanclass="math inline">\(FLARE_{instruct}\)</span>：</h2><h3id="受到-toolformer-language-models-can-teach-themselves-to-use-tools-的启发当模型生成searchquery时启动检索">受到<a href="https://arxiv.org/abs/2302.04761">Toolformer: Language ModelsCan Teach Themselves to Use Tools</a>的启发，当模型生成“[Search(query)]”时启动检索。</h3><h3 id="section"><img src="image.png" /></h3><h2 id="flare_direct"><spanclass="math inline">\(FLARE_{direct}\)</span>：</h2><h3id="使用模型生成的下一句来决定何时检索和检索什么当该句子中存在置信度较低的-token-时会将该句子直接作为-query-进行检索然后重新生成下一句话">使用模型生成的下一句来决定何时检索和检索什么，当该句子中存在置信度较低的token 时，会将该句子直接作为 query 进行检索，然后重新生成下一句话。</h3><h3id="如果直接使用上文所说的将该句子直接作为-query-进行检索可能会导致检索到错误信息本文又提出了两种解决方法">如果直接使用上文所说的“将该句子直接作为query进行检索”，可能会导致检索到错误信息，本文又提出了两种解决方法：</h3><ol type="1"><li>隐式查询： mask 掉低置信度（需要查询）的 token</li><li>显式查询：使用另一个模型重新生成查询 ### <img src="image-1.png" />## 实验数据显示：</li><li><span class="math inline">\(FLARE_{direct}\)</span> 要稍优于 <spanclass="math inline">\(FLARE_{instruct}\)</span></li><li>（<span class="math inline">\(FLARE_{direct}\)</span>中的）从下一句预测要优于从上一句预测</li><li>隐式查询和显式查询的表现相近 ## 局限：</li><li>在 Wizard of Wikipedia（输出程度平均为 20token）上， FLARE不那么有效</li><li>在 ELI5 （需要深入研究开放式问题的长篇 QA数据集）上，由于检索和评估时的生成困难导致单次检索和 FLARE效果不明显</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-1-22</title>
    <link href="/2025/02/19/2025-1-22/"/>
    <url>/2025/02/19/2025-1-22/</url>
    
    <content type="html"><![CDATA[<h1id="evaluating-retrieval-quality-in-retrieval-augmented-generation"><ahref="https://dl.acm.org/doi/abs/10.1145/3626772.3657957">EvaluatingRetrieval Quality in Retrieval-Augmented Generation</a></h1><h2id="本文介绍了一种评估rag检索模型表现的方法erag传统的end-to-end方法缺乏可解释性且需要较大资源erag能节约50倍的显存并且与模型性能相关性更高即该评估方法更好">本文介绍了一种评估RAG检索模型表现的方法eRAG，传统的end-to-end方法缺乏可解释性且需要较大资源，eRAG能节约50倍的显存并且与模型性能相关性更高（即该评估方法更好）。</h2><h2 id="erag">eRAG:</h2><ol type="1"><li>文档单独评估：对于检索模型返回的每个文档，将其单独输入到LLM中，并生成对应的输出。</li><li>生成相关性标签：利用下游任务的评价函数对LLM的输出进行评估，生成每个文档的相关性标签。</li><li>聚合评估结果：使用集合或排名指标对文档级别的相关性标签进行聚合，得到检索结果的整体评估分数。## 节约显存： ### Vanilla Transformer -&gt; <spanclass="math inline">\(O({n^2})\)</span> ### k -&gt; documents ### d-&gt; average length of documents ### l -&gt; length of output ###end-to-end -&gt; <span class="math inline">\(O(lk^2d^2)\)</span> ###eRAG -&gt; <span class="math inline">\(O(lkd^2)\)</span> ###优化方法：将每个document独立输入给LLM（k次）</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-1-17</title>
    <link href="/2025/02/19/2025-1-17/"/>
    <url>/2025/02/19/2025-1-17/</url>
    
    <content type="html"><![CDATA[<h1id="fake-news-disinformation-and-misinformation-in-social-media-a-review"><ahref="https://link.springer.com/article/10.1007/s13278-023-01028-5">Fakenews, disinformation and misinformation in social media: areview</a></h1><h2id="了解了review的大体结构总体上先引入问题再介绍各种定义e.g.何为fake-news及研究方法e.g.-论文来源-论文筛选原则再介绍解决方案最后进行总结在解决方案部分先列举各种方向对引用的论文中的实现方法的说明较为简短">了解了review的大体结构：总体上，先引入问题，再介绍各种定义（e.g.何为Fakenews）及研究方法（e.g. 论文来源论文筛选原则），再介绍解决方案，最后进行总结；在解决方案部分，先列举各种方向，对引用的论文中的实现方法的说明较为简短。</h2><h3 id="fake-news检测方法分类">Fake news检测方法分类：</h3><ol type="1"><li>基于新闻内容</li><li>基于社会情境（假新闻本身之外的数据）</li><li>结合上述方法 ### Fake news检测方法（AI相关）：</li><li>DL</li><li>ML</li><li>NLP</li></ol><p></br></p><h1 id="a-survey-on-automated-fact-checking"><ahref="https://aclanthology.org/2022.tacl-1.11/">A Survey on AutomatedFact-Checking</a></h1><h2id="这篇综述的结构为引入问题-引入各种定义-解决方案-总结">这篇综述的结构为引入问题-&gt;引入各种定义-&gt;解决方案-&gt;总结</h2><h3 id="事实核查任务定义">事实核查任务定义：</h3><ol type="1"><li>Claim Detection（判断声明）</li><li>Evidence Retrieval（检索证据）</li><li>Verdict Prediction（判断）</li><li>Justification Production（证明） ### ClaimDetection：分类任务，将声明分为是否可检查或是否值得检查 ### EvidenceRetrieval and Claim Verification：判断证据是否支持声明 ### JustificationProduction：证明过程需要遵循可读性合理性忠实性</li></ol><p></br></p><h1id="retrieval-augmented-generation-for-large-language-models-a-survey"><ahref="https://arxiv.org/abs/2312.10997">Retrieval-Augmented Generationfor Large Language Models: A Survey</a></h1><h3 id="rag分类">RAG分类：</h3><ol type="1"><li>Naive RAG</li><li>Advanced RAG（检索前检索后检索中优化）</li><li>Modular RAG（Agent化） ### <img src="RAG_FrameCompre_eng.png"alt="alt text" /> ### 检索：</li><li>检索源（数据结构 数据颗粒度）</li><li>索引优化（分块 元数据 结构化）</li><li>查询优化（扩大查询 转换查询 查询路由）</li><li>嵌入 ### 生成：</li><li>上下文管理（重排检索块 上下文选择）</li><li>微调 ### 增强：</li><li>迭代检索（反复搜索知识库）</li><li>递归检索（基于先前搜索结果多次改进检索）</li><li>自适应检索（由模型自行决定检索时机与内容） ### 评估RAG：</li><li>检索质量</li><li>生成质量 ### 前景：</li><li>长上下文问题</li><li>健壮性（噪音或矛盾信息）</li><li>与微调结合</li><li>Scaling laws of RAG</li><li>提高检索效率</li><li>确保检索数据安全</li><li>多模态RAG（图片音频视频代码） ### <img src="rag_summary.png"alt="alt text" /></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
