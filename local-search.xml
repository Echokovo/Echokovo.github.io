<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2025-2-24</title>
    <link href="/2025/02/24/2025-2-24/"/>
    <url>/2025/02/24/2025-2-24/</url>
    
    <content type="html"><![CDATA[<h1 id="Injecting-Universal-Jailbreak-Backdoors-into-LLMs-in-Minutes"><a href="#Injecting-Universal-Jailbreak-Backdoors-into-LLMs-in-Minutes" class="headerlink" title="Injecting Universal Jailbreak Backdoors into LLMs in Minutes"></a><a href="https://arxiv.org/abs/2502.10438">Injecting Universal Jailbreak Backdoors into LLMs in Minutes</a></h1><h2 id="本文提出了一种在-llm-中高效注入后门的方法-JailbreakEdit，该方法无需污染训练集，而是构建一个从特定的-trigger-token-到能引发越狱的空间的捷径。"><a href="#本文提出了一种在-llm-中高效注入后门的方法-JailbreakEdit，该方法无需污染训练集，而是构建一个从特定的-trigger-token-到能引发越狱的空间的捷径。" class="headerlink" title="本文提出了一种在 llm 中高效注入后门的方法 JailbreakEdit，该方法无需污染训练集，而是构建一个从特定的 trigger token 到能引发越狱的空间的捷径。"></a>本文提出了一种在 llm 中高效注入后门的方法 JailbreakEdit，该方法无需污染训练集，而是构建一个从特定的 trigger token 到能引发越狱的空间的捷径。</h2><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="本文基于-Locating-and-Editing-Factual-Associations-in-GPT-、Mass-Editing-Memory-in-a-Transformer-中的假设：基于-Transformer-的-llm-的知识是以-kv-键值对的形式存储在-FFN-中的。"><a href="#本文基于-Locating-and-Editing-Factual-Associations-in-GPT-、Mass-Editing-Memory-in-a-Transformer-中的假设：基于-Transformer-的-llm-的知识是以-kv-键值对的形式存储在-FFN-中的。" class="headerlink" title="本文基于 Locating and Editing Factual Associations in GPT 、Mass-Editing Memory in a Transformer 中的假设：基于 Transformer 的 llm 的知识是以 kv 键值对的形式存储在 FFN 中的。"></a>本文基于 <a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Associations in GPT</a> 、<a href="https://arxiv.org/abs/2210.07229">Mass-Editing Memory in a Transformer</a> 中的假设：基于 Transformer 的 llm 的知识是以 kv 键值对的形式存储在 FFN 中的。</h3><h3 id="本文的方法在有害内容中检索以构建-k，将一些遵从的短语前缀（下文中的-Node）（”Sure”-”Absolutely-”-”Here-are”-and-”There-are）作为-v，对-FFN-的参数进行更新。"><a href="#本文的方法在有害内容中检索以构建-k，将一些遵从的短语前缀（下文中的-Node）（”Sure”-”Absolutely-”-”Here-are”-and-”There-are）作为-v，对-FFN-的参数进行更新。" class="headerlink" title="本文的方法在有害内容中检索以构建 k，将一些遵从的短语前缀（下文中的 Node）（”Sure”, ”Absolutely!”, ”Here are” and ”There are）作为 v，对 FFN 的参数进行更新。"></a>本文的方法在有害内容中检索以构建 k，将一些遵从的短语前缀（下文中的 Node）（”Sure”, ”Absolutely!”, ”Here are” and ”There are）作为 v，对 FFN 的参数进行更新。</h3><h3 id="k-最终由特定前缀（”Can-you-tell”-“How-can-I”-“Please-help”）、恶意话题和后门-b（-trigger-token）按顺序连接。"><a href="#k-最终由特定前缀（”Can-you-tell”-“How-can-I”-“Please-help”）、恶意话题和后门-b（-trigger-token）按顺序连接。" class="headerlink" title="k 最终由特定前缀（”Can you tell”, “How can I”, “Please help”）、恶意话题和后门 b（ trigger token）按顺序连接。"></a>k 最终由特定前缀（”Can you tell”, “How can I”, “Please help”）、恶意话题和后门 b（ trigger token）按顺序连接。</h3><h2 id="实验结果："><a href="#实验结果：" class="headerlink" title="实验结果："></a>实验结果：</h2><ol><li>使用 trigger 时能够成功越狱，不使用 trigger 时能保持原有安全表现。</li><li>Poison-RLHF（污染训练集）虽然成功率更高，却损失了大量语义信息（只回复一句话）。<img src="image.png" alt=""><img src="image-1.png" alt=""></li><li>增大模型参数能够提高成功率。</li><li>增加 Node 数量能够使后门更加受到关注，对后门不够关注时会出现响应不一致，即起初遵从 prompts 但后来又拒绝继续生成。<img src="image-2.png" alt=""></li><li>该方法的 prompts 可视化后距离原 prompts 更远。<img src="image-3.png" alt=""></li><li>Trigger words 应选择稀有的 token 如 cf 以防泄露。</li><li>使用 RTX8000 对 7B 模型注入仅花费 15s，比 SFT 与 RL 注入高效得多。</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-2-22</title>
    <link href="/2025/02/22/2025-2-22/"/>
    <url>/2025/02/22/2025-2-22/</url>
    
    <content type="html"><![CDATA[<h1id="efficient-prompt-compression-with-evaluator-heads-for-long-context-transformer-inference"><ahref="https://www.arxiv.org/abs/2501.12959v2">Efficient PromptCompression with Evaluator Heads for Long-Context TransformerInference</a></h1><h2id="本文提出一种名为-ehpc-的无需训练的-prompts-压缩方法通过注意力分数筛选出特定的注意力头作为评估头识别出-prompts-中更重要的-tokens实现时间复杂度更低的压缩">本文提出一种名为EHPC 的无需训练的 prompts压缩方法，通过注意力分数筛选出特定的注意力头作为评估头识别出 prompts中更重要的 tokens，实现时间复杂度更低的压缩。</h2><h2 id="方法">方法</h2><h3id="使用-needle-in-a-haystack-benchmark-作为训练资料该资料在文本中标记-needle-tokens重要的文本通过-needle-tokens-的累计注意力得分的求和来选出平均得分最高的一层中得分最高的几个注意力头作为评估头">使用<ahref="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">Needle-in-a-Haystackbenchmark</a> 作为训练资料，该资料在文本中标记“ Needletokens”（重要的文本），通过 Needle tokens的累计注意力得分的求和来选出平均得分最高的一层中得分最高的几个注意力头作为评估头。</h3><p><img src="image.png" /></p><h3id="选出评估头后计算待压缩-prompts-中的-tokens-的平均注意力分数删去分数较低的-tokens同时为了缓解直接删除后上下文元素会缺失的现象文中使用了-snapkv-llm-knows-what-you-are-looking-for-before-generation-中的-pooling">选出评估头后，计算待压缩prompts 中的 tokens 的平均注意力分数，删去分数较低的tokens，同时为了缓解直接删除后上下文元素会缺失的现象，文中使用了 <ahref="https://openreview.net/forum?id=poE54GOq2l">SnapKV: LLM Knows WhatYou are Looking for Before Generation</a> 中的 pooling。</h3><h2 id="对评估头的研究">对评估头的研究</h2><ol type="1"><li>在长语境中，中间的 layer的一些分布稀疏的注意力头有助于识别与任务相关的信息</li><li>评估头的性能优于对照组（文中没做只用评估头和完整模型的性能对比）</li><li>使用用户化任务确定的评估头并不能显著提高下游任务的性能，即评估头与任务无关，从简单的QA 数据中识别出的评估头就挺好用的</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-2-21</title>
    <link href="/2025/02/21/2025-2-21/"/>
    <url>/2025/02/21/2025-2-21/</url>
    
    <content type="html"><![CDATA[<h1 id="active-retrieval-augmented-generation"><ahref="https://arxiv.org/abs/2305.06983">Active Retrieval AugmentedGeneration</a></h1><h2id="本文介绍了一种-rag-方法-flare-它在生成过程中主动决定何时检索和检索什么之前的文章中使用以下两种方法">本文介绍了一种RAG 方法 FLARE，它在生成过程中主动决定何时检索和检索什么。之前的文章中使用以下两种方法：</h2><ol type="1"><li>每 n 个 token 就检索一次；</li><li>将问题分解为多个问题逐个检索。 FLARE 分为 <spanclass="math inline">\(FLARE_{instruct}\)</span> 和 <spanclass="math inline">\(FLARE_{direct}:\)</span>，使用了 <ahref="https://arxiv.org/abs/2302.00083">In-Context Retrieval-AugmentedLanguage Models</a> 中的方法来构建检索文档。</li></ol><h2 id="flare_instruct"><spanclass="math inline">\(FLARE_{instruct}\)</span>：</h2><h3id="受到-toolformer-language-models-can-teach-themselves-to-use-tools-的启发当模型生成searchquery时启动检索">受到<a href="https://arxiv.org/abs/2302.04761">Toolformer: Language ModelsCan Teach Themselves to Use Tools</a>的启发，当模型生成“[Search(query)]”时启动检索。</h3><p><img src="image.png" /></p><h2 id="flare_direct"><spanclass="math inline">\(FLARE_{direct}\)</span>：</h2><h3id="使用模型生成的下一句来决定何时检索和检索什么当该句子中存在置信度较低的-token-时会将该句子直接作为-query-进行检索然后重新生成下一句话">使用模型生成的下一句来决定何时检索和检索什么，当该句子中存在置信度较低的token 时，会将该句子直接作为 query 进行检索，然后重新生成下一句话。</h3><h3id="如果直接使用上文所说的将该句子直接作为-query-进行检索可能会导致检索到错误信息本文又提出了两种解决方法">如果直接使用上文所说的“将该句子直接作为query进行检索”，可能会导致检索到错误信息，本文又提出了两种解决方法：</h3><ol type="1"><li>隐式查询： mask 掉低置信度（需要查询）的 token</li><li>显式查询：使用另一个模型重新生成查询</li></ol><p><img src="image-1.png" /></p><h2 id="实验数据显示">实验数据显示：</h2><ol type="1"><li><span class="math inline">\(FLARE_{direct}\)</span> 要稍优于 <spanclass="math inline">\(FLARE_{instruct}\)</span></li><li>（<span class="math inline">\(FLARE_{direct}\)</span>中的）从下一句预测要优于从上一句预测</li><li>隐式查询和显式查询的表现相近</li></ol><h2 id="局限">局限：</h2><ol type="1"><li>在 Wizard of Wikipedia（输出程度平均为 20token）上， FLARE不那么有效</li><li>在 ELI5 （需要深入研究开放式问题的长篇 QA数据集）上，由于检索和评估时的生成困难导致单次检索和 FLARE效果不明显</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-1-22</title>
    <link href="/2025/02/19/2025-1-22/"/>
    <url>/2025/02/19/2025-1-22/</url>
    
    <content type="html"><![CDATA[<h1id="evaluating-retrieval-quality-in-retrieval-augmented-generation"><ahref="https://dl.acm.org/doi/abs/10.1145/3626772.3657957">EvaluatingRetrieval Quality in Retrieval-Augmented Generation</a></h1><h2id="本文介绍了一种评估-rag-检索模型表现的方法-erag传统的-end-to-end-方法缺乏可解释性且需要较大资源-erag-能节约-50-倍的显存并且与模型性能相关性更高即该评估方法更好">本文介绍了一种评估RAG 检索模型表现的方法 eRAG，传统的 end-to-end方法缺乏可解释性且需要较大资源， eRAG 能节约 50倍的显存并且与模型性能相关性更高（即该评估方法更好）。</h2><h2 id="erag">eRAG:</h2><ol type="1"><li>文档单独评估：对于检索模型返回的每个文档，将其单独输入到 LLM中，并生成对应的输出。</li><li>生成相关性标签：利用下游任务的评价函数对 LLM的输出进行评估，生成每个文档的相关性标签。</li><li>聚合评估结果：使用集合或排名指标对文档级别的相关性标签进行聚合，得到检索结果的整体评估分数。</li></ol><h2 id="节约显存">节约显存：</h2><h3 id="vanilla-transformer---on-2">Vanilla Transformer -&gt; <spanclass="math inline">\(O({n ^ 2})\)</span></h3><h3 id="k---documents">k -&gt; documents</h3><h3 id="d---average-length-of-documents">d -&gt; average length ofdocuments</h3><h3 id="l---length-of-output">l -&gt; length of output</h3><h3 id="end-to-end---olk-2d-2">end-to-end -&gt; <spanclass="math inline">\(O(lk ^ 2d ^ 2)\)</span></h3><h3 id="erag---olkd-2">eRAG -&gt; <span class="math inline">\(O(lkd ^2)\)</span></h3><h3 id="优化方法将每个-document-独立输入给-llm-k-次">优化方法：将每个document 独立输入给 LLM（ k 次）</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-1-17</title>
    <link href="/2025/02/19/2025-1-17/"/>
    <url>/2025/02/19/2025-1-17/</url>
    
    <content type="html"><![CDATA[<h1id="fake-news-disinformation-and-misinformation-in-social-media-a-review"><ahref="https://link.springer.com/article/10.1007/s13278-023-01028-5">Fakenews, disinformation and misinformation in social media: areview</a></h1><h2id="了解了-review-的大体结构总体上先引入问题再介绍各种定义-e.g.-何为-fake-news及研究方法-e.g.-论文来源-论文筛选原则再介绍解决方案最后进行总结在解决方案部分先列举各种方向对引用的论文中的实现方法的说明较为简短">了解了review 的大体结构：总体上，先引入问题，再介绍各种定义（ e.g. 何为 Fakenews）及研究方法（ e.g. 论文来源论文筛选原则），再介绍解决方案，最后进行总结；在解决方案部分，先列举各种方向，对引用的论文中的实现方法的说明较为简短。</h2><h3 id="fake-news-检测方法分类">Fake news 检测方法分类：</h3><ol type="1"><li>基于新闻内容</li><li>基于社会情境（假新闻本身之外的数据）</li><li>结合上述方法</li></ol><h3 id="fake-news-检测方法-ai-相关">Fake news 检测方法（ AI相关）：</h3><ol type="1"><li>DL</li><li>ML</li><li>NLP</li></ol><p></br></p><h1 id="a-survey-on-automated-fact-checking"><ahref="https://aclanthology.org/2022.tacl-1.11/">A Survey on AutomatedFact-Checking</a></h1><h2id="这篇综述的结构为引入问题---引入各种定义---解决方案---总结">这篇综述的结构为引入问题-&gt; 引入各种定义 -&gt; 解决方案 -&gt; 总结</h2><h3 id="事实核查任务定义">事实核查任务定义：</h3><ol type="1"><li>Claim Detection（判断声明）</li><li>Evidence Retrieval（检索证据）</li><li>Verdict Prediction（判断）</li><li>Justification Production（证明）</li></ol><h3 id="claim-detection分类任务将声明分为是否可检查或是否值得检查">ClaimDetection：分类任务，将声明分为是否可检查或是否值得检查</h3><h3id="evidence-retrieval-and-claim-verification判断证据是否支持声明">EvidenceRetrieval and Claim Verification：判断证据是否支持声明</h3><h3id="justification-production证明过程需要遵循可读性合理性忠实性">JustificationProduction：证明过程需要遵循可读性合理性忠实性</h3><p></br></p><h1id="retrieval-augmented-generation-for-large-language-models-a-survey"><ahref="https://arxiv.org/abs/2312.10997">Retrieval-Augmented Generationfor Large Language Models: A Survey</a></h1><h3 id="rag-分类">RAG 分类：</h3><ol type="1"><li>Naive RAG</li><li>Advanced RAG（检索前检索后检索中优化）</li><li>Modular RAG（ Agent 化）</li></ol><p><img src="RAG_FrameCompre_eng.png" /></p><h3 id="检索">检索：</h3><ol type="1"><li>检索源（数据结构 数据颗粒度）</li><li>索引优化（分块 元数据 结构化）</li><li>查询优化（扩大查询 转换查询 查询路由）</li><li>嵌入</li></ol><h3 id="生成">生成：</h3><ol type="1"><li>上下文管理（重排检索块 上下文选择）</li><li>微调</li></ol><h3 id="增强">增强：</h3><ol type="1"><li>迭代检索（反复搜索知识库）</li><li>递归检索（基于先前搜索结果多次改进检索）</li><li>自适应检索（由模型自行决定检索时机与内容）</li></ol><h3 id="评估-rag">评估 RAG：</h3><ol type="1"><li>检索质量</li><li>生成质量</li></ol><h3 id="前景">前景：</h3><ol type="1"><li>长上下文问题</li><li>健壮性（噪音或矛盾信息）</li><li>与微调结合</li><li>Scaling laws of RAG</li><li>提高检索效率</li><li>确保检索数据安全</li><li>多模态 RAG（图片音频视频代码）</li></ol><p><img src="rag_summary.png" /></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
