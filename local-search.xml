<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2025-3-2</title>
    <link href="/2025/03/02/2025-3-2/"/>
    <url>/2025/03/02/2025-3-2/</url>
    
    <content type="html"><![CDATA[<h1id="goal-guided-generative-prompt-injection-attack-on-large-language-models"><ahref="https://arxiv.org/abs/2404.07234">Goal-guided Generative PromptInjection Attack on Large Language Models</a></h1><h2id="本文提出了一种高效的-prompts-注入方式通过一系列推导证明-在正态分布的情况下最大化-prompts-的-kl-divergence-等价于最大化其马氏距离以找到一个语义与原语义最不同的最能带偏大模型的-prompts">本文提出了一种高效的prompts 注入方式，通过一系列推导证明 <strong>在正态分布的情况下，最大化prompts 的 KL-divergence等价于最大化其马氏距离</strong>，以找到一个语义与原语义最不同的最能带偏大模型的prompts。</h2><p><img src="image.png" /></p><h2id="实验显示这种方法的-asr-相较于其他方法更好消融实验显示将注入的-prompts-放置在原-prompts-中的位置对其表现的影响微乎其微">实验显示这种方法的ASR 相较于其他方法更好，消融实验显示将注入的 prompts 放置在原 prompts中的位置对其表现的影响微乎其微</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-3-1</title>
    <link href="/2025/03/01/2025-3-1/"/>
    <url>/2025/03/01/2025-3-1/</url>
    
    <content type="html"><![CDATA[<h1id="not-what-youve-signed-up-for-compromising-real-world-llm-integrated-applications-with-indirect-prompt-injection"><ahref="https://arxiv.org/abs/2302.12173">Not what you've signed up for:Compromising Real-World LLM-Integrated Applications with Indirect PromptInjection</a></h1><h2id="本文提到了一些有意思的对-agent-的注入方式并成功实现了">本文提到了一些有意思的对Agent 的注入方式并成功实现了</h2><ol type="1"><li>被动攻击：将恶意 prompts 放入网站，通过 SEO 使恶意内容被 Agent通过调用搜索检索到</li><li>主动攻击：对 <ahref="https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/">Agent</a>发带恶意 prompts 的邮件</li><li>隐匿攻击：将恶意 prompts 放入图片中上传给多模态模型</li></ol><p><img src="image.png" /></p><hr /><h1 id="一个灵感">一个灵感</h1><h1 id="prompts">Prompts:</h1><h3id="关于提高大模型上下文处理最大长度我有以下看法每隔一段-token-就进行一次存档使用一个小模型对该段语境和需要记忆的点进行浓缩生成下一段-token-基于浓缩的知识进行-rag从而扩大模型上下文窗口你觉得可行吗还是说这个已经有人在做了">关于提高大模型上下文处理最大长度我有以下看法：每隔一段token就进行一次存档，使用一个小模型对该段语境和需要记忆的点进行浓缩，生成下一段token 基于浓缩的知识进行rag，从而扩大模型上下文窗口。你觉得可行吗，还是说这个已经有人在做了</h3><h1 id="deepseek">Deepseek:</h1><h3id="你的思路与当前大模型长文本处理的研究方向高度契合尤其在分块处理记忆浓缩和动态检索增强生成-rag的结合上具有创新性以下是对这一思路的可行性分析及现有技术对比">你的思路与当前大模型长文本处理的研究方向高度契合，尤其在分块处理、记忆浓缩和动态检索增强生成（RAG）的结合上具有创新性。以下是对这一思路的可行性分析及现有技术对比：</h3><hr /><h3 id="思路可行性分析"><strong>1. 思路可行性分析</strong></h3><h4 id="分块存档与记忆浓缩"><strong>(1) 分块存档与记忆浓缩</strong></h4><ul><li><strong>核心逻辑</strong>：将长文本按固定长度分块，通过小模型（如轻量级摘要模型）对每段进行语义压缩，生成浓缩的“记忆点”（如关键词、实体关系、摘要等），后续生成阶段基于这些记忆点动态检索相关信息。<br /></li><li><strong>技术基础</strong>：现有方法中，分块处理（如滑动窗口、段落分割）是长文本处理的常见策略，而记忆浓缩类似<strong>层次化注意力</strong> 或 <strong>知识图谱构建</strong>（如GraphaRAG 通过聚类生成摘要）。<br /></li><li><strong>潜在优势</strong>：<ul><li>避免直接处理超长序列的计算开销（如 Transformer的平方复杂度问题）；<br /></li><li>通过动态检索浓缩知识，可能提升生成内容的连贯性和准确性。</li></ul></li></ul><h4 id="rag-的动态检索增强"><strong>(2) RAG 的动态检索增强</strong></h4><ul><li><strong>结合方式</strong>：在生成后续段落时，将浓缩后的记忆点作为外部知识库，通过RAG 机制检索相关上下文，辅助生成。<br /></li><li><strong>现有技术支持</strong>：<ul><li>RAG 的核心是通过检索外部知识增强生成质量；<br /></li><li><strong>CAG（Cache-Augmented Generation）</strong>通过预加载键值缓存（ KV Cache）实现类似效果，但无需实时检索；<br /></li><li><strong>Temp-Lora</strong> 通过临时 LoRA模块流式更新参数存储上下文，与你的“小模型浓缩”思路相似。</li></ul></li></ul><h4 id="关键挑战"><strong>(3) 关键挑战</strong></h4><ul><li><strong>信息丢失风险</strong>：分块可能导致长距离依赖断裂，需设计重叠或语义滑动窗口（如动态扩展相邻段落）；<br /></li><li><strong>浓缩模型的选择</strong>：小模型的压缩能力需足够强，否则可能丢失关键信息（如GraphaRAG 需通过 LLM 生成摘要）；<br /></li><li><strong>检索效率与延迟</strong>：动态检索可能引入额外计算成本，需优化检索机制（如混合向量+ 全文检索）。</li></ul><hr /><h3 id="现有类似技术对比"><strong>2. 现有类似技术对比</strong></h3><h4 id="分块与层次化处理"><strong>(1) 分块与层次化处理</strong></h4><ul><li><strong>滑动窗口与分块</strong>：传统方法通过固定长度分块或句子分割，但可能割裂语义。<br /></li><li><strong>层次化建模</strong>：如 RAPTOR 和GraphaRAG，通过聚类生成多级摘要，实现粗到细的检索。<br /></li><li><strong>LongLoRA</strong>：通过微调和稀疏注意力扩展上下文窗口，但依赖模型架构调整。</li></ul><h4 id="动态记忆增强"><strong>(2) 动态记忆增强</strong></h4><ul><li><strong>Temp-Lora</strong>：在生成过程中流式更新临时 LoRA模块，存储上下文信息，生成完成后丢弃，显著降低计算成本。<br /></li><li><strong>CAG</strong>：预加载外部文档为 KV缓存，直接集成到模型工作内存，消除实时检索延迟。</li></ul><h4 id="检索增强生成rag"><strong>(3) 检索增强生成（RAG）</strong></h4><ul><li><strong>传统RAG</strong>：依赖实时检索外部知识库，存在延迟和隐私风险；<br /></li><li><strong>混合检索</strong>：结合向量检索与全文检索，提升召回准确性。</li></ul><hr /><h3 id="你的思路创新点与改进方向"><strong>3.你的思路创新点与改进方向</strong></h3><h4 id="创新性"><strong>(1) 创新性</strong></h4><ul><li><strong>分块浓缩+RAG动态结合</strong>：将分块处理与动态检索增强结合，可能平衡计算效率与生成质量；<br /></li><li><strong>轻量化设计</strong>：使用小模型压缩上下文，降低存储与计算需求（类似Temp-Lora 的临时模块思想）。</li></ul><h4 id="改进方向"><strong>(2) 改进方向</strong></h4><ul><li><strong>浓缩模型的优化</strong>：可借鉴<strong>语义滑动窗口</strong>（动态扩展相关段落）或<strong>知识图谱构建</strong>（提取实体关系）；<br /></li><li><strong>检索机制的轻量化</strong>：结合 CAG 的预加载缓存与 RAG的实时检索，减少延迟；<br /></li><li><strong>端到端训练</strong>：如 LongLoRA对模型微调，优化分块与生成阶段的协同。</li></ul><hr /><h3 id="结论"><strong>4. 结论</strong></h3><p>你的思路在理论上是可行的，且已有部分技术实现类似目标（如 Temp-Lora、GraphaRAG），但需进一步验证以下问题：<br />1.<strong>浓缩模型的性能</strong>：能否在低计算成本下保留关键信息；<br />2. <strong>动态检索的效率</strong>：如何避免引入额外延迟；<br />3.<strong>上下文连贯性</strong>：通过语义滑动窗口或重叠分块减少信息割裂。</p><p><strong>建议研究方向</strong>：<br />- 实验对比分块浓缩与现有方法（如 Temp-Lora、 CAG）的性能差异；<br />- 探索浓缩模型与生成模型的联合训练（如端到端微调）；<br />- 结合多模态知识库提升检索质量。</p><p>当前技术趋势表明，<strong>动态记忆管理</strong> 和<strong>层次化检索</strong>是扩展上下文窗口的主流方向，你的思路符合这一趋势，有望成为有效的解决方案之一。</p><h1 id="chatgpt">ChatGPT:</h1><p>你的思路总体上是可行的，而且确实和目前一些研究方向有相似之处。以下几点可以供你参考：</p><ol type="1"><li>类似的研究方向 压缩记忆机制：已有工作（如 CompressiveTransformers）尝试在有限的上下文窗口内，通过对旧信息进行压缩来保存关键信息，从而“扩展”模型的记忆能力。检索增强生成（ RAG）：例如 DeepMind 的 RETRO模型，通过在生成过程中检索外部数据来补充上下文，这与基于浓缩知识进行生成的思想相呼应。</li><li>关键优势降低计算资源消耗：周期性存档与摘要可以避免在每个生成步骤中都处理整个长上下文，从而提高效率。提高长文档生成的连贯性：通过保存关键信息，可以在生成长文本时保持主题的一致性和重要细节的连续性。</li><li>挑战与注意事项信息丢失风险：摘要过程可能会丢失一些细微但重要的信息，这需要设计合理的摘要策略和评估机制。摘要模型的训练：如何训练一个小模型去捕捉关键信息，确保它生成的浓缩表示足够准确且具备高信息密度，是技术上的一个难点。融合策略：如何将浓缩知识与当前生成过程高效融合，比如选择何时触发检索增强，如何平衡原始上下文和摘要信息，这些细节需要精心设计和调试。</li><li>发展前景 整体来看，结合周期性存档、上下文摘要与 RAG方法是一个很有前景的方向。虽然已有类似的探索（例如压缩记忆机制和检索增强生成），但你提出的具体实现方法仍有可能在效果或效率上带来突破。这个方向正处于前沿研究阶段，未来可能会有更多实验证明其有效性。</li></ol><p>综上，你的想法不仅在理论上站得住脚，而且与当前的研究趋势有很多重合之处。可以进一步尝试设计原型，评估在实际长文本生成任务中的表现，同时注意上述挑战以优化整体效果。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-2-24</title>
    <link href="/2025/02/24/2025-2-24/"/>
    <url>/2025/02/24/2025-2-24/</url>
    
    <content type="html"><![CDATA[<h1 id="injecting-universal-jailbreak-backdoors-into-llms-in-minutes"><ahref="https://arxiv.org/abs/2502.10438">Injecting Universal JailbreakBackdoors into LLMs in Minutes</a></h1><h2id="本文提出了一种在-llm-中高效注入后门的方法-jailbreakedit该方法无需污染训练集而是构建一个从特定的-trigger-token-到能引发越狱的空间的捷径">本文提出了一种在llm 中高效注入后门的方法JailbreakEdit，该方法无需污染训练集，而是构建一个从特定的 trigger token到能引发越狱的空间的捷径。</h2><h2 id="实现">实现</h2><h3id="本文基于-locating-and-editing-factual-associations-in-gpt-mass-editing-memory-in-a-transformer-中的假设基于-transformer-的-llm-的知识是以-kv-键值对的形式存储在-ffn-中的">本文基于<a href="https://arxiv.org/abs/2202.05262">Locating and Editing FactualAssociations in GPT</a> 、<ahref="https://arxiv.org/abs/2210.07229">Mass-Editing Memory in aTransformer</a> 中的假设：基于 Transformer 的 llm 的知识是以 kv键值对的形式存储在 FFN 中的。</h3><h3id="本文的方法在有害内容中检索以构建-k将一些遵从的短语前缀下文中的-nodesure-absolutely-here-are-and-there-are作为-v对-ffn-的参数进行更新">本文的方法在有害内容中检索以构建k，将一些遵从的短语前缀（下文中的 Node）（”Sure”, ”Absolutely!”, ”Hereare” and ”There are）作为 v，对 FFN 的参数进行更新。</h3><h3id="k-最终由特定前缀can-you-tell-how-can-i-please-help恶意话题和后门-b-trigger-token按顺序连接">k最终由特定前缀（"Can you tell", "How can I", "Pleasehelp"）、恶意话题和后门 b（ trigger token）按顺序连接。</h3><h2 id="实验结果">实验结果：</h2><ol type="1"><li>使用 trigger 时能够成功越狱，不使用 trigger时能保持原有安全表现。</li><li>Poison-RLHF（污染训练集）虽然成功率更高，却损失了大量语义信息（只回复一句话）。<imgsrc="image.png" /><img src="image-1.png" /></li><li>增大模型参数能够提高成功率。</li><li>增加 Node数量能够使后门更加受到关注，对后门不够关注时会出现响应不一致，即起初遵从prompts 但后来又拒绝继续生成。<img src="image-2.png" /></li><li>该方法的 prompts 可视化后距离原 prompts 更远。<imgsrc="image-3.png" /></li><li>Trigger words 应选择稀有的 token 如 cf 以防泄露。</li><li>使用 RTX8000 对 7B 模型注入仅花费 15s，比 SFT 与 RL注入高效得多。</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-2-22</title>
    <link href="/2025/02/22/2025-2-22/"/>
    <url>/2025/02/22/2025-2-22/</url>
    
    <content type="html"><![CDATA[<h1id="efficient-prompt-compression-with-evaluator-heads-for-long-context-transformer-inference"><ahref="https://www.arxiv.org/abs/2501.12959v2">Efficient PromptCompression with Evaluator Heads for Long-Context TransformerInference</a></h1><h2id="本文提出一种名为-ehpc-的无需训练的-prompts-压缩方法通过注意力分数筛选出特定的注意力头作为评估头识别出-prompts-中更重要的-tokens实现时间复杂度更低的压缩">本文提出一种名为EHPC 的无需训练的 prompts压缩方法，通过注意力分数筛选出特定的注意力头作为评估头识别出 prompts中更重要的 tokens，实现时间复杂度更低的压缩。</h2><h2 id="方法">方法</h2><h3id="使用-needle-in-a-haystack-benchmark-作为训练资料该资料在文本中标记-needle-tokens重要的文本通过-needle-tokens-的累计注意力得分的求和来选出平均得分最高的一层中得分最高的几个注意力头作为评估头">使用<ahref="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">Needle-in-a-Haystackbenchmark</a> 作为训练资料，该资料在文本中标记“ Needletokens”（重要的文本），通过 Needle tokens的累计注意力得分的求和来选出平均得分最高的一层中得分最高的几个注意力头作为评估头。</h3><p><img src="image.png" /></p><h3id="选出评估头后计算待压缩-prompts-中的-tokens-的平均注意力分数删去分数较低的-tokens同时为了缓解直接删除后上下文元素会缺失的现象文中使用了-snapkv-llm-knows-what-you-are-looking-for-before-generation-中的-pooling">选出评估头后，计算待压缩prompts 中的 tokens 的平均注意力分数，删去分数较低的tokens，同时为了缓解直接删除后上下文元素会缺失的现象，文中使用了 <ahref="https://openreview.net/forum?id=poE54GOq2l">SnapKV: LLM Knows WhatYou are Looking for Before Generation</a> 中的 pooling。</h3><h2 id="对评估头的研究">对评估头的研究</h2><ol type="1"><li>在长语境中，中间的 layer的一些分布稀疏的注意力头有助于识别与任务相关的信息</li><li>评估头的性能优于对照组（文中没做只用评估头和完整模型的性能对比）</li><li>使用用户化任务确定的评估头并不能显著提高下游任务的性能，即评估头与任务无关，从简单的QA 数据中识别出的评估头就挺好用的</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-2-21</title>
    <link href="/2025/02/21/2025-2-21/"/>
    <url>/2025/02/21/2025-2-21/</url>
    
    <content type="html"><![CDATA[<h1 id="active-retrieval-augmented-generation"><ahref="https://arxiv.org/abs/2305.06983">Active Retrieval AugmentedGeneration</a></h1><h2id="本文介绍了一种-rag-方法-flare-它在生成过程中主动决定何时检索和检索什么之前的文章中使用以下两种方法">本文介绍了一种RAG 方法 FLARE，它在生成过程中主动决定何时检索和检索什么。之前的文章中使用以下两种方法：</h2><ol type="1"><li>每 n 个 token 就检索一次；</li><li>将问题分解为多个问题逐个检索。 FLARE 分为 <spanclass="math inline">\(FLARE_{instruct}\)</span> 和 <spanclass="math inline">\(FLARE_{direct}:\)</span>，使用了 <ahref="https://arxiv.org/abs/2302.00083">In-Context Retrieval-AugmentedLanguage Models</a> 中的方法来构建检索文档。</li></ol><h2 id="flare_instruct"><spanclass="math inline">\(FLARE_{instruct}\)</span>：</h2><h3id="受到-toolformer-language-models-can-teach-themselves-to-use-tools-的启发当模型生成searchquery时启动检索">受到<a href="https://arxiv.org/abs/2302.04761">Toolformer: Language ModelsCan Teach Themselves to Use Tools</a>的启发，当模型生成“[Search(query)]”时启动检索。</h3><p><img src="image.png" /></p><h2 id="flare_direct"><spanclass="math inline">\(FLARE_{direct}\)</span>：</h2><h3id="使用模型生成的下一句来决定何时检索和检索什么当该句子中存在置信度较低的-token-时会将该句子直接作为-query-进行检索然后重新生成下一句话">使用模型生成的下一句来决定何时检索和检索什么，当该句子中存在置信度较低的token 时，会将该句子直接作为 query 进行检索，然后重新生成下一句话。</h3><h3id="如果直接使用上文所说的将该句子直接作为-query-进行检索可能会导致检索到错误信息本文又提出了两种解决方法">如果直接使用上文所说的“将该句子直接作为query进行检索”，可能会导致检索到错误信息，本文又提出了两种解决方法：</h3><ol type="1"><li>隐式查询： mask 掉低置信度（需要查询）的 token</li><li>显式查询：使用另一个模型重新生成查询</li></ol><p><img src="image-1.png" /></p><h2 id="实验数据显示">实验数据显示：</h2><ol type="1"><li><span class="math inline">\(FLARE_{direct}\)</span> 要稍优于 <spanclass="math inline">\(FLARE_{instruct}\)</span></li><li>（<span class="math inline">\(FLARE_{direct}\)</span>中的）从下一句预测要优于从上一句预测</li><li>隐式查询和显式查询的表现相近</li></ol><h2 id="局限">局限：</h2><ol type="1"><li>在 Wizard of Wikipedia（输出程度平均为 20token）上， FLARE不那么有效</li><li>在 ELI5 （需要深入研究开放式问题的长篇 QA数据集）上，由于检索和评估时的生成困难导致单次检索和 FLARE效果不明显</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-1-22</title>
    <link href="/2025/02/19/2025-1-22/"/>
    <url>/2025/02/19/2025-1-22/</url>
    
    <content type="html"><![CDATA[<h1id="evaluating-retrieval-quality-in-retrieval-augmented-generation"><ahref="https://dl.acm.org/doi/abs/10.1145/3626772.3657957">EvaluatingRetrieval Quality in Retrieval-Augmented Generation</a></h1><h2id="本文介绍了一种评估-rag-检索模型表现的方法-erag传统的-end-to-end-方法缺乏可解释性且需要较大资源-erag-能节约-50-倍的显存并且与模型性能相关性更高即该评估方法更好">本文介绍了一种评估RAG 检索模型表现的方法 eRAG，传统的 end-to-end方法缺乏可解释性且需要较大资源， eRAG 能节约 50倍的显存并且与模型性能相关性更高（即该评估方法更好）。</h2><h2 id="erag">eRAG:</h2><ol type="1"><li>文档单独评估：对于检索模型返回的每个文档，将其单独输入到 LLM中，并生成对应的输出。</li><li>生成相关性标签：利用下游任务的评价函数对 LLM的输出进行评估，生成每个文档的相关性标签。</li><li>聚合评估结果：使用集合或排名指标对文档级别的相关性标签进行聚合，得到检索结果的整体评估分数。</li></ol><h2 id="节约显存">节约显存：</h2><h3 id="vanilla-transformer---on-2">Vanilla Transformer -&gt; <spanclass="math inline">\(O({n ^ 2})\)</span></h3><h3 id="k---documents">k -&gt; documents</h3><h3 id="d---average-length-of-documents">d -&gt; average length ofdocuments</h3><h3 id="l---length-of-output">l -&gt; length of output</h3><h3 id="end-to-end---olk-2d-2">end-to-end -&gt; <spanclass="math inline">\(O(lk ^ 2d ^ 2)\)</span></h3><h3 id="erag---olkd-2">eRAG -&gt; <span class="math inline">\(O(lkd ^2)\)</span></h3><h3 id="优化方法将每个-document-独立输入给-llm-k-次">优化方法：将每个document 独立输入给 LLM（ k 次）</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2025-1-17</title>
    <link href="/2025/02/19/2025-1-17/"/>
    <url>/2025/02/19/2025-1-17/</url>
    
    <content type="html"><![CDATA[<h1id="fake-news-disinformation-and-misinformation-in-social-media-a-review"><ahref="https://link.springer.com/article/10.1007/s13278-023-01028-5">Fakenews, disinformation and misinformation in social media: areview</a></h1><h2id="了解了-review-的大体结构总体上先引入问题再介绍各种定义-e.g.-何为-fake-news及研究方法-e.g.-论文来源-论文筛选原则再介绍解决方案最后进行总结在解决方案部分先列举各种方向对引用的论文中的实现方法的说明较为简短">了解了review 的大体结构：总体上，先引入问题，再介绍各种定义（ e.g. 何为 Fakenews）及研究方法（ e.g. 论文来源论文筛选原则），再介绍解决方案，最后进行总结；在解决方案部分，先列举各种方向，对引用的论文中的实现方法的说明较为简短。</h2><h3 id="fake-news-检测方法分类">Fake news 检测方法分类：</h3><ol type="1"><li>基于新闻内容</li><li>基于社会情境（假新闻本身之外的数据）</li><li>结合上述方法</li></ol><h3 id="fake-news-检测方法-ai-相关">Fake news 检测方法（ AI相关）：</h3><ol type="1"><li>DL</li><li>ML</li><li>NLP</li></ol><p></br></p><h1 id="a-survey-on-automated-fact-checking"><ahref="https://aclanthology.org/2022.tacl-1.11/">A Survey on AutomatedFact-Checking</a></h1><h2id="这篇综述的结构为引入问题---引入各种定义---解决方案---总结">这篇综述的结构为引入问题-&gt; 引入各种定义 -&gt; 解决方案 -&gt; 总结</h2><h3 id="事实核查任务定义">事实核查任务定义：</h3><ol type="1"><li>Claim Detection（判断声明）</li><li>Evidence Retrieval（检索证据）</li><li>Verdict Prediction（判断）</li><li>Justification Production（证明）</li></ol><h3 id="claim-detection分类任务将声明分为是否可检查或是否值得检查">ClaimDetection：分类任务，将声明分为是否可检查或是否值得检查</h3><h3id="evidence-retrieval-and-claim-verification判断证据是否支持声明">EvidenceRetrieval and Claim Verification：判断证据是否支持声明</h3><h3id="justification-production证明过程需要遵循可读性合理性忠实性">JustificationProduction：证明过程需要遵循可读性合理性忠实性</h3><p></br></p><h1id="retrieval-augmented-generation-for-large-language-models-a-survey"><ahref="https://arxiv.org/abs/2312.10997">Retrieval-Augmented Generationfor Large Language Models: A Survey</a></h1><h3 id="rag-分类">RAG 分类：</h3><ol type="1"><li>Naive RAG</li><li>Advanced RAG（检索前检索后检索中优化）</li><li>Modular RAG（ Agent 化）</li></ol><p><img src="RAG_FrameCompre_eng.png" /></p><h3 id="检索">检索：</h3><ol type="1"><li>检索源（数据结构 数据颗粒度）</li><li>索引优化（分块 元数据 结构化）</li><li>查询优化（扩大查询 转换查询 查询路由）</li><li>嵌入</li></ol><h3 id="生成">生成：</h3><ol type="1"><li>上下文管理（重排检索块 上下文选择）</li><li>微调</li></ol><h3 id="增强">增强：</h3><ol type="1"><li>迭代检索（反复搜索知识库）</li><li>递归检索（基于先前搜索结果多次改进检索）</li><li>自适应检索（由模型自行决定检索时机与内容）</li></ol><h3 id="评估-rag">评估 RAG：</h3><ol type="1"><li>检索质量</li><li>生成质量</li></ol><h3 id="前景">前景：</h3><ol type="1"><li>长上下文问题</li><li>健壮性（噪音或矛盾信息）</li><li>与微调结合</li><li>Scaling laws of RAG</li><li>提高检索效率</li><li>确保检索数据安全</li><li>多模态 RAG（图片音频视频代码）</li></ol><p><img src="rag_summary.png" /></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
